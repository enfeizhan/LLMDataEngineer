{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb287ca6-a5e5-4bb6-8022-e6cd2019029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dbb1a67-a2ab-48a0-a62f-36fa070d8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa87220-fc6a-48ae-8424-9dcb5d35a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''An organization is creating a data lake on AWS and requires granular access control. They need to grant specific users access to certain rows and columns within their datasets. The organization's teams will query the data using a combination of Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR. Which AWS service should the organization implement to manage data permissions efficiently?\n",
    "\n",
    "A. Manage access through S3 bucket policies and IAM roles for row and column-level security.\n",
    "\n",
    "B. Deploy Apache Ranger on Amazon EMR for granular access control and utilize Amazon Redshift for querying.\n",
    "\n",
    "C. Use Redshift security groups and views for row and column-level permissions, querying with Athena and Redshift Spectrum.\n",
    "\n",
    "D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d184df4-b234-4607-ac1d-03cd70c1d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, model, temperature=0):\n",
    "    llm = ChatGroq(model=model, temperature=temperature)\n",
    "    return llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ea7ff-48eb-4e6a-94bf-8cc6fa7c19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma2_9b_it = answer_question(question, model='gemma2-9b-it')\n",
    "print(gemma2_9b_it.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b85dda-2b52-484b-b870-b02e0daae95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.**\n",
      "\n",
      "AWS Lake Formation provides a centralized and granular data access control mechanism for data lakes on AWS. It allows organizations to:\n",
      "\n",
      "- Define fine-grained permissions at the row and column level.\n",
      "- Control access to specific datasets or tables.\n",
      "- Grant access only to authorized users and groups.\n",
      "- Support querying data using Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR.\n",
      "\n",
      "By using Lake Formation, the organization can efficiently manage data permissions and ensure that only authorized users have access to the rows and columns they need.\n"
     ]
    }
   ],
   "source": [
    "gemma_7b_it = answer_question(question, model='gemma-7b-it')\n",
    "print(gemma_7b_it.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295d6a1b-d3df-4e5e-94fe-ee3404a055ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct answer is:\n",
      "\n",
      "D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.\n",
      "\n",
      "AWS Lake Formation is a data warehousing and analytics service that makes it easy to set up, secure, and manage data lakes. It provides fine-grained access control, allowing administrators to grant access to specific rows and columns within datasets. Lake Formation integrates with various AWS services, including Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR, making it an ideal solution for managing data permissions efficiently.\n",
      "\n",
      "The other options are not the best choices because:\n",
      "\n",
      "A. While S3 bucket policies and IAM roles can provide some level of access control, they may not offer the same level of granularity as AWS Lake Formation.\n",
      "\n",
      "B. Apache Ranger on Amazon EMR can provide granular access control, but it may require additional setup and management, and it may not integrate as seamlessly with other AWS services.\n",
      "\n",
      "C. Redshift security groups and views can provide some level of access control, but they may not be suitable for managing access to data lakes, and they may not integrate with other AWS services like Athena and EMR.\n",
      "\n",
      "Therefore, using AWS Lake Formation is the most efficient and effective way to manage data permissions for the organization's data lake.\n"
     ]
    }
   ],
   "source": [
    "llama_3_3_70b_versatile = answer_question(question, model='llama-3.3-70b-versatile')\n",
    "print(llama_3_3_70b_versatile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5ebbce-c236-4cfb-962f-b82c816dfa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct answer is D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.\n",
      "\n",
      "AWS Lake Formation is a fully managed service that makes it easy to set up a secure data lake and manage data permissions efficiently. It provides fine-grained access control, allowing you to grant specific users access to certain rows and columns within your datasets. Lake Formation supports querying through Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR, making it an ideal solution for this organization's requirements.\n",
      "\n",
      "Here's why the other options are incorrect:\n",
      "\n",
      "A. While S3 bucket policies and IAM roles can provide some level of access control, they are not designed to manage row and column-level security.\n",
      "\n",
      "B. Apache Ranger is a popular open-source project for managing access control, but it is not a native AWS service. Deploying Ranger on Amazon EMR would require additional setup and maintenance.\n",
      "\n",
      "C. Redshift security groups and views can provide some level of access control, but they are not designed to manage row and column-level security across multiple datasets and services.\n",
      "\n",
      "In summary, AWS Lake Formation is the most suitable solution for managing granular access control in a data lake on AWS, especially when querying through multiple services like Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR.\n"
     ]
    }
   ],
   "source": [
    "llama_3_1_8b_instant = answer_question(question, model='llama-3.1-8b-instant')\n",
    "print(llama_3_1_8b_instant.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66134664-6bd5-49cb-ad58-776c0edf99c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.\n",
      "\n",
      "AWS Lake Formation is the best choice for managing granular access control in this scenario. It allows you to define fine-grained data access policies and apply them across your data lake, including Amazon S3, Amazon Redshift, and Amazon EMR. This way, you can efficiently manage permissions for specific users to access certain rows and columns within your datasets. Additionally, Lake Formation supports querying through Athena, Redshift Spectrum, and EMR, making it a versatile solution for your needs.\n",
      "\n",
      "While other options might provide some level of access control, they lack the ability to manage permissions at the row and column level efficiently across multiple querying services.\n"
     ]
    }
   ],
   "source": [
    "mixtral_8_7b_32768 = answer_question(question, model='mixtral-8x7b-32768')\n",
    "print(mixtral_8_7b_32768.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b6d5b37-a223-48d0-af87-73b5885f578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = '''Answer the following exam question and at the end back all your explanations with searchable references:\n",
    "A company is migrating its database servers from Amazon EC2 instances that run Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. The company's analytics team must export large data elements every day until the migration is complete. The data elements are the result of SQL joins across multiple tables. The data must be in Apache Parquet format. The analytics team must store the data in Amazon S3.\n",
    "Which solution will meet these requirements in the MOST operationally efficient way?\n",
    "\n",
    "A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
    "B. Schedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet.\n",
    "C. Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
    "D. Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41f83478-4014-41b9-af3c-8d016e65a9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MOST operationally efficient solution is **A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.**\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Efficiency:**\n",
      "\n",
      "    * **Direct Data Access:**  AWS Glue jobs can directly query views, eliminating the need for intermediate steps like CSV conversion or Lambda function execution. This reduces processing time and resource consumption.\n",
      "    * **Parquet Format:**  Parquet is a columnar storage format optimized for analytical workloads. AWS Glue can efficiently read and write Parquet data, further enhancing performance.\n",
      "\n",
      "* **Simplicity:**\n",
      "\n",
      "    * **Minimal Components:** Solution A involves the fewest moving parts: a view, an AWS Glue job, and an S3 bucket. This simplifies management and reduces the potential for errors.\n",
      "    * **Established Tools:** AWS Glue and views are well-established tools within the AWS ecosystem, making implementation straightforward.\n",
      "\n",
      "* **Scalability:**\n",
      "\n",
      "    * **AWS Glue:** AWS Glue is a serverless data integration service that scales automatically based on workload demands. This ensures your data export process can handle increasing data volumes.\n",
      "\n",
      "**Searchable References:**\n",
      "\n",
      "* **AWS Glue:** https://aws.amazon.com/glue/\n",
      "* **Parquet Format:** https://parquet.apache.org/\n",
      "* **Views in SQL Server:** https://docs.microsoft.com/en-us/sql/t-sql/statements/create-view-transact-sql?view=sql-server-ver16\n",
      "\n",
      "\n",
      "Let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gemma2_9b_it = answer_question(question2, model='gemma2-9b-it')\n",
    "print(gemma2_9b_it.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85614e0f-50ae-4220-8b50-43e38032cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The most operationally efficient solution is C.**\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Option A** is not efficient because it involves creating and managing a view in the source database, which can impact performance. Additionally, scheduling an AWS Glue job to run every day can be resource-intensive.\n",
      "\n",
      "\n",
      "* **Option B** is complex and involves multiple steps. Scheduling SQL Server Agent can be challenging, and converting CSV files to Parquet format using Lambda can be inefficient.\n",
      "\n",
      "\n",
      "* **Option C** is the most efficient solution because it uses AWS Glue, which is specifically designed for data extraction, transformation, and loading (ETL) tasks. Creating a Glue crawler to read the view and then running a Glue job to export the data in Parquet format to S3 is a streamlined process.\n",
      "\n",
      "\n",
      "* **Option D** is not recommended because it involves using Lambda to perform database connectivity, which can be less efficient than using a dedicated ETL tool like AWS Glue. Additionally, managing JDBC connections in Lambda can be more complex.\n",
      "\n",
      "**References:**\n",
      "\n",
      "* **AWS Glue Documentation:** https://docs.aws.amazon.com/glue/latest/userguide/what-is-glue.html\n",
      "* **AWS Lambda Documentation:** https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n",
      "* **AWS EventBridge Documentation:** https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-events.html\n",
      "\n",
      "**Therefore, the recommended solution is to use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.**\n"
     ]
    }
   ],
   "source": [
    "gemma_7b_it = answer_question(question2, model='gemma-7b-it')\n",
    "print(gemma_7b_it.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cb8e837-642f-40fe-a809-0af59f31810b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most operationally efficient solution to meet the requirements is:\n",
      "\n",
      "A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
      "\n",
      "This solution is the most efficient for several reasons:\n",
      "\n",
      "1. **Direct Data Access**: By creating a view in the SQL Server databases, the AWS Glue job can directly access the required data elements, reducing the need for additional data processing or transformation steps.\n",
      "2. **Optimized Data Transfer**: AWS Glue is optimized for data transfer and processing, making it the most efficient choice for transferring large data elements to S3.\n",
      "3. **Native Parquet Support**: AWS Glue natively supports Parquet format, eliminating the need for additional data transformation steps.\n",
      "4. **Scheduling**: Scheduling the AWS Glue job to run every day ensures that the data is exported and transferred to S3 in a timely and consistent manner.\n",
      "\n",
      "The other options are less efficient due to the following reasons:\n",
      "\n",
      "* Option B requires an additional transformation step from CSV to Parquet, which adds complexity and processing time.\n",
      "* Option C requires creating and running an AWS Glue crawler, which adds an extra step and may not be necessary if the data is already defined in a view.\n",
      "* Option D requires using Java Database Connectivity (JDBC) and an AWS Lambda function, which may introduce additional complexity and latency compared to using AWS Glue.\n",
      "\n",
      "References:\n",
      "\n",
      "* AWS Glue: [https://aws.amazon.com/glue/](https://aws.amazon.com/glue/)\n",
      "* AWS Glue Jobs: [https://docs.aws.amazon.com/glue/latest/dg/glue-jobs.html](https://docs.aws.amazon.com/glue/latest/dg/glue-jobs.html)\n",
      "* Apache Parquet: [https://parquet.apache.org/](https://parquet.apache.org/)\n",
      "* AWS Lambda: [https://aws.amazon.com/lambda/](https://aws.amazon.com/lambda/)\n",
      "* Amazon EventBridge: [https://aws.amazon.com/eventbridge/](https://aws.amazon.com/eventbridge/)\n",
      "* AWS Glue Crawler: [https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html)\n",
      "* SQL Server Agent: [https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15](https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15)\n"
     ]
    }
   ],
   "source": [
    "llama_3_3_70b_versatile = answer_question(question2, model='llama-3.3-70b-versatile')\n",
    "print(llama_3_3_70b_versatile.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be328015-acfd-4235-a119-70b1c458435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most operationally efficient solution is D. Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "1. **Direct Access**: Lambda function can directly access the EC2 instance-based databases using JDBC, eliminating the need for additional infrastructure or data transfer.\n",
      "2. **Efficient Data Processing**: Lambda function can process the data in real-time, transforming it into Parquet format and transferring it to S3 without the need for intermediate steps or data storage.\n",
      "3. **Scalability**: Lambda function can scale automatically to handle large data volumes, ensuring that the data export process is efficient and reliable.\n",
      "4. **Cost-Effective**: Using Lambda function eliminates the need for additional infrastructure, such as AWS Glue jobs or SQL Server Agent, reducing costs associated with data export.\n",
      "\n",
      "Here's a breakdown of the other options:\n",
      "\n",
      "A. **Inefficient Data Transfer**: Creating a view in the EC2 instance-based SQL Server databases and using AWS Glue job to transfer data to S3 involves multiple steps, including data transfer from EC2 to Glue, and then from Glue to S3.\n",
      "\n",
      "B. **Intermediate Data Storage**: Using SQL Server Agent to export data to S3 and then using an AWS Lambda function to transform the data format involves intermediate data storage, which can lead to increased costs and complexity.\n",
      "\n",
      "C. **Overkill**: Using an AWS Glue crawler to read the view and then creating an AWS Glue job to transfer data to S3 involves unnecessary complexity and additional infrastructure, making it less efficient than option D.\n",
      "\n",
      "References:\n",
      "\n",
      "* [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)\n",
      "* [Amazon EventBridge](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-what-is.html)\n",
      "* [AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)\n",
      "* [SQL Server Agent](https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15)\n",
      "* [Apache Parquet](https://parquet.apache.org/)\n",
      "* [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)\n"
     ]
    }
   ],
   "source": [
    "llama_3_1_8b_instant = answer_question(question2, model='llama-3.1-8b-instant')\n",
    "print(llama_3_1_8b_instant.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d4d4fe5-7113-4fb6-96de-32e914ef381c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C. Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
      "\n",
      "Explanation:\n",
      "\n",
      "This solution meets the requirements of exporting large data elements in Apache Parquet format from EC2 instances running Microsoft SQL Server to Amazon S3. By creating a view in the EC2 instance-based SQL Server databases, you can simplify the data extraction process and ensure that the data is up-to-date.\n",
      "\n",
      "AWS Glue is a fully managed ETL service that makes it easy to move data between data stores. In this scenario, using AWS Glue to extract data from the SQL Server view, convert it to Apache Parquet format, and store it in Amazon S3 is the most operationally efficient way.\n",
      "\n",
      "Here's a breakdown of the steps involved in this solution:\n",
      "\n",
      "1. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements.\n",
      "2. Create and run an AWS Glue crawler to read the view. This will discover the data schema and create a catalog in the AWS Glue Data Catalog.\n",
      "3. Create an AWS Glue job that retrieves the data from the catalog and converts it to Apache Parquet format.\n",
      "4. Schedule the AWS Glue job to run every day.\n",
      "\n",
      "This solution ensures that the data is extracted in a structured and automated way, reducing the risk of errors and ensuring that the analytics team has access to the latest data.\n",
      "\n",
      "References:\n",
      "\n",
      "* AWS Glue: <https://aws.amazon.com/glue/>\n",
      "* AWS Glue Data Catalog: <https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-data-catalog.html>\n",
      "* AWS Glue Jobs: <https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-jobs.html>\n",
      "* AWS Glue Crawlers: <https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-crawlers.html>\n",
      "* AWS Glue Scheduling: <https://docs.aws.amazon.com/glue/latest/dg/schedule-job.html>\n",
      "* AWS Glue and SQL Server: <https://aws.amazon.com/blogs/big-data/using-aws-glue-to-extract-data-from-sql-server/>\n",
      "* AWS Glue and Parquet: <https://aws.amazon.com/blogs/big-data/using-aws-glue-to-convert-data-to-parquet-format/>\n"
     ]
    }
   ],
   "source": [
    "mixtral_8_7b_32768 = answer_question(question2, model='mixtral-8x7b-32768')\n",
    "print(mixtral_8_7b_32768.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df1f9a-80e1-4b12-9546-eb614120aa44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
