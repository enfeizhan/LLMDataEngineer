{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6dS0Yg-iCEpE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC7T4fYCymsY",
    "outputId": "7490a759-90c2-4a6c-a4d2-5c7ea2936400"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''An organization is creating a data lake on AWS and requires granular access control. They need to grant specific users access to certain rows and columns within their datasets. The organization's teams will query the data using a combination of Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR. Which AWS service should the organization implement to manage data permissions efficiently?\n",
    "\n",
    "A. Manage access through S3 bucket policies and IAM roles for row and column-level security.\n",
    "\n",
    "B. Deploy Apache Ranger on Amazon EMR for granular access control and utilize Amazon Redshift for querying.\n",
    "\n",
    "C. Use Redshift security groups and views for row and column-level permissions, querying with Athena and Redshift Spectrum.\n",
    "\n",
    "D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq34ir65cIPK",
    "outputId": "f1f728f7-c3e3-478e-a064-1f89f2fe5b61"
   },
   "outputs": [],
   "source": [
    "def answer_question(question, model, temperature=0):\n",
    "    llm = ChatOpenAI(model=model, temperature=temperature)\n",
    "    return llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SgEIIpagZpcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.\n",
      "\n",
      "AWS Lake Formation is designed to simplify the process of setting up a secure data lake and provides fine-grained access control for data stored in Amazon S3. It allows you to define and enforce granular access policies at the database, table, column, and row levels. These policies can be applied across various AWS analytics services, including Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR, making it the most suitable choice for managing data permissions efficiently in this scenario.\n"
     ]
    }
   ],
   "source": [
    "gpt_4o = answer_question(question, model='gpt-4o')\n",
    "print(gpt_4o.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SgEIIpagZpcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best option for the organization to manage data permissions efficiently, especially for granular access control at the row and column level, is:\n",
      "\n",
      "**D. Use AWS Lake Formation to define fine-grained data access policies and facilitate queries through supported AWS services.**\n",
      "\n",
      "AWS Lake Formation is specifically designed to simplify the process of setting up a data lake and provides fine-grained access control capabilities. It allows you to define permissions at the table, column, and row levels, which is essential for the organization's requirement to grant specific users access to certain rows and columns within their datasets. Additionally, Lake Formation integrates seamlessly with services like Amazon Athena, Amazon Redshift Spectrum, and Apache Hive on Amazon EMR, making it a suitable choice for the organization's querying needs.\n"
     ]
    }
   ],
   "source": [
    "gpt_4o_mini = answer_question(question, model='gpt-4o-mini')\n",
    "print(gpt_4o_mini.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = '''Answer the following exam question and at the end back all your explanations with searchable references:\n",
    "A company is migrating its database servers from Amazon EC2 instances that run Microsoft SQL Server to Amazon RDS for Microsoft SQL Server DB instances. The company's analytics team must export large data elements every day until the migration is complete. The data elements are the result of SQL joins across multiple tables. The data must be in Apache Parquet format. The analytics team must store the data in Amazon S3.\n",
    "Which solution will meet these requirements in the MOST operationally efficient way?\n",
    "\n",
    "A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
    "B. Schedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet.\n",
    "C. Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.\n",
    "D. Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SgEIIpagZpcJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the most operationally efficient solution for migrating data from Amazon EC2 instances running Microsoft SQL Server to Amazon RDS for Microsoft SQL Server while exporting large data elements in Apache Parquet format to Amazon S3, we need to evaluate the options based on their operational overhead, ease of implementation, and ability to meet the requirements.\n",
      "\n",
      "### Analysis of Options:\n",
      "\n",
      "**A. Create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create an AWS Glue job that selects the data directly from the view and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.**\n",
      "- **Pros:** AWS Glue is designed for ETL (Extract, Transform, Load) processes and can easily convert data to Parquet format. Scheduling the job is straightforward.\n",
      "- **Cons:** Requires setting up and managing AWS Glue, which may introduce some complexity.\n",
      "\n",
      "**B. Schedule SQL Server Agent to run a daily SQL query that selects the desired data elements from the EC2 instance-based SQL Server databases. Configure the query to direct the output .csv objects to an S3 bucket. Create an S3 event that invokes an AWS Lambda function to transform the output format from .csv to Parquet.**\n",
      "- **Pros:** Utilizes SQL Server Agent, which is familiar to SQL Server administrators.\n",
      "- **Cons:** This approach involves multiple steps (exporting to CSV, triggering a Lambda function, and transforming the data), which adds complexity and potential points of failure. Additionally, the transformation from CSV to Parquet may not be as efficient as direct extraction.\n",
      "\n",
      "**C. Use a SQL query to create a view in the EC2 instance-based SQL Server databases that contains the required data elements. Create and run an AWS Glue crawler to read the view. Create an AWS Glue job that retrieves the data and transfers the data in Parquet format to an S3 bucket. Schedule the AWS Glue job to run every day.**\n",
      "- **Pros:** Similar to option A, but adds the use of a Glue crawler, which may not be necessary if the view is already defined. This could add unnecessary complexity.\n",
      "- **Cons:** The additional step of using a crawler may not be needed, making this option less efficient.\n",
      "\n",
      "**D. Create an AWS Lambda function that queries the EC2 instance-based databases by using Java Database Connectivity (JDBC). Configure the Lambda function to retrieve the required data, transform the data into Parquet format, and transfer the data into an S3 bucket. Use Amazon EventBridge to schedule the Lambda function to run every day.**\n",
      "- **Pros:** This option allows for a fully serverless architecture and can be efficient for smaller datasets. However, it may require more coding and management of the Lambda function.\n",
      "- **Cons:** Lambda has limitations on execution time and memory, which may not be suitable for large datasets resulting from SQL joins.\n",
      "\n",
      "### Conclusion:\n",
      "\n",
      "**The best option is A.** It provides a straightforward and efficient way to extract data directly from a SQL Server view, transform it into Parquet format using AWS Glue, and store it in S3. This minimizes operational overhead and complexity while meeting the requirements effectively.\n",
      "\n",
      "### References:\n",
      "1. AWS Glue Documentation: [AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)\n",
      "2. Amazon S3 Documentation: [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)\n",
      "3. SQL Server Agent Documentation: [SQL Server Agent](https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15)\n",
      "4. AWS Lambda Documentation: [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)\n"
     ]
    }
   ],
   "source": [
    "gpt_4o_mini = answer_question(question2, model='gpt-4o-mini')\n",
    "print(gpt_4o_mini.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the most operationally efficient solution for exporting large data elements from Microsoft SQL Server databases on Amazon EC2 to Amazon S3 in Apache Parquet format, we need to consider the capabilities and integration of AWS services, as well as the ease of automation and maintenance.\n",
      "\n",
      "**Option A:**\n",
      "- **Explanation:** This option suggests creating a view in the SQL Server databases and using an AWS Glue job to select data from the view and transfer it to S3 in Parquet format. AWS Glue is a fully managed ETL (Extract, Transform, Load) service that can efficiently handle data transformation and loading tasks. By using a view, the complexity of SQL joins is abstracted, making the Glue job simpler. Scheduling the Glue job to run daily automates the process.\n",
      "- **Operational Efficiency:** AWS Glue is designed for such ETL tasks and can handle large data volumes efficiently. It also natively supports writing data in Parquet format.\n",
      "- **References:** \n",
      "  - AWS Glue: [AWS Glue Documentation](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html)\n",
      "  - Parquet format support: [AWS Glue Supported Formats](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format.html)\n",
      "\n",
      "**Option B:**\n",
      "- **Explanation:** This option involves using SQL Server Agent to export data to CSV format and then using an S3 event to trigger a Lambda function to convert CSV to Parquet. This introduces additional steps and dependencies, such as managing CSV files and ensuring the Lambda function can handle the conversion efficiently.\n",
      "- **Operational Efficiency:** This approach is less efficient due to the intermediate CSV format and the need for a separate conversion step, which can be error-prone and resource-intensive.\n",
      "- **References:** \n",
      "  - SQL Server Agent: [SQL Server Agent Documentation](https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15)\n",
      "  - AWS Lambda: [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)\n",
      "\n",
      "**Option C:**\n",
      "- **Explanation:** Similar to Option A, this option uses a view and AWS Glue but adds a Glue crawler to read the view. The Glue job then transfers the data to S3 in Parquet format. The crawler is used to infer the schema, which might not be necessary if the schema is already known.\n",
      "- **Operational Efficiency:** While efficient, the use of a crawler adds an unnecessary step if the schema is static, slightly reducing operational efficiency compared to Option A.\n",
      "- **References:** \n",
      "  - AWS Glue Crawler: [AWS Glue Crawlers](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)\n",
      "\n",
      "**Option D:**\n",
      "- **Explanation:** This option involves using a Lambda function with JDBC to query the databases, transform the data to Parquet, and store it in S3. While Lambda can be used for such tasks, it is not ideal for handling large data volumes due to its execution time and memory limits.\n",
      "- **Operational Efficiency:** This approach is less efficient for large datasets due to Lambda's limitations and the complexity of handling JDBC connections and Parquet transformations within Lambda.\n",
      "- **References:** \n",
      "  - AWS Lambda Limits: [AWS Lambda Limits](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html)\n",
      "\n",
      "**Conclusion:**\n",
      "Option A is the most operationally efficient solution. It leverages AWS Glue's capabilities to handle large data volumes and perform the necessary transformations directly to Parquet format, minimizing intermediate steps and manual interventions. AWS Glue's integration with S3 and its ability to schedule jobs make it well-suited for this task.\n",
      "\n",
      "**Selected Option:** A\n"
     ]
    }
   ],
   "source": [
    "gpt_4o = answer_question(question2, model='gpt-4o')\n",
    "print(gpt_4o.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
