{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6dS0Yg-iCEpE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from glob import glob\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4k0jCK-wDByV"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObTM2Xi-DKWb"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rC7T4fYCymsY",
    "outputId": "7490a759-90c2-4a6c-a4d2-5c7ea2936400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7NUMsRgDL6H"
   },
   "outputs": [],
   "source": [
    "choices_desc = '''\n",
    "The letter(s) representing the correct choice(s) out of the multiple\n",
    "given choices from a question in an exam.\n",
    "'''\n",
    "enum = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    '''Answer to an exam.'''\n",
    "    choices: str = Field(\n",
    "        description=choices_desc,\n",
    "        enum=enum\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dLdXaMxDOdj"
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        'system',\n",
    "        f'You are in the {exam} exam.'\n",
    "        ' You must answer the multiple-choice question.'\n",
    "    ),\n",
    "    ('user', '{question}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zM53JkDjHdcw",
    "outputId": "c06fcc35-d188-496a-954b-e0b37668f3d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are in a GCP Professional Data Engineer exam. You must answer the multiple-choice question.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4o-mini', 'gpt-4o']\n",
    "exams = glob('examQuestions/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['examQuestions/DP-900AzureDataFundamentals.csv',\n",
       " 'examQuestions/DP-203AzureDataEngineerAssociat.csv',\n",
       " 'examQuestions/GCPProfessionalDataEngineer.csv',\n",
       " 'examQuestions/AWSDataEngineerAssociate.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eq34ir65cIPK",
    "outputId": "f1f728f7-c3e3-478e-a064-1f89f2fe5b61"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for model in models:\n",
    "    results[model] = []\n",
    "    model = ChatOpenAI(\n",
    "        model=model,\n",
    "        temperature=0\n",
    "    ).with_structured_output(Answer)\n",
    "    chain = prompt_template | model\n",
    "    for exam in exams:\n",
    "        exam_key = exam.replace('examQuestions/', '').replace('.csv', '')\n",
    "        results[exam_key] = []\n",
    "        ques = pd.read_csv(exam, encoding='latin1')\n",
    "        for question in tqdm(ques.Questions):\n",
    "            result = chain.invoke({'question': question}).choices.replace(',', '').replace(' ', '')\n",
    "            results[exam_key].append(result)\n",
    "            sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "SBfm_4HyWpIc",
    "outputId": "2cc33e26-35e7-4bde-c283-e715ea8dcb27"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'llamaapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bddcaa53b9e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllamaapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llamaapi'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llamaapi import LlamaAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzMRykFPYrG4"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZudSRsxYtT4"
   },
   "outputs": [],
   "source": [
    "api_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7NGa66hYvTE"
   },
   "outputs": [],
   "source": [
    "llama = LlamaAPI(api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwR3b9vYYyg8"
   },
   "outputs": [],
   "source": [
    "# API Request JSON Cell\n",
    "api_request_json = {\n",
    "  \"model\": \"llama3.1-70b\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": f\"You are in a {exam} exam. You must answer the multiple-choice question. The possible answers are only from A, B, C, D, or E.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thUhuGnvY2AZ",
    "outputId": "240f99c0-ec0b-47c2-c6ff-f811a1bb2b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created\": 1732238247,\n",
      "  \"model\": \"llama3.1-70b\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 231,\n",
      "    \"completion_tokens\": 124,\n",
      "    \"total_tokens\": 355\n",
      "  },\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The correct answer is A. Use AWS Glue to convert the .csv data to Apache Parquet and partition it by timestamp.\\n\\nExplanation:\\nAWS Glue is a fully managed extract, transform, and load (ETL) service that can handle the transformation of the .csv data into Apache Parquet format, which is more efficient for analytics. Partitioning the data by timestamp will also improve query performance. AWS Glue is a cost-effective solution as it is a serverless service, and you only\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\",\n",
      "        \"function_call\": null,\n",
      "        \"tool_calls\": null\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Make your request and handle the response\n",
    "response = llama.run(api_request_json)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt_hZ5sjY4jj"
   },
   "outputs": [],
   "source": [
    "# Build the Request\n",
    "api_request_json = {\n",
    "    'model': 'llama3.1-8b',\n",
    "    'functions': [\n",
    "        {\n",
    "            \"name\": \"Person\", ## Function name referenced in function_call\n",
    "            \"description\": \"Extract information about a person.\", ## Function description\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": { ## Structure of the properties you expect to return as an object\n",
    "                    \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "                    \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"}\n",
    "                },\n",
    "            \"required\": [\"name\", \"age\"]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    'function_call': {'name': 'Person'}, ## Pass your function\n",
    "    'messages': [\n",
    "        {'role': 'user', 'content': \"John is 23 years old.\"}],\n",
    "  }\n",
    "\n",
    "# Execute the Request\n",
    "response = llama.run(api_request_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfZ6HEmUiYQy"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5EMpKS_q_C4"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = api_token,\n",
    "    base_url = \"https://api.llama-api.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7qER7IrrYjv"
   },
   "outputs": [],
   "source": [
    "def get_answer(model, question):\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    max_tokens=128,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"You are in a {exam} exam. You must only return the letter(s) representing the recommended option(s). Strictly no explanations or any necessary text output. There can be one or more correct options. Normally the question will give hints.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    tools = [\n",
    "      {\n",
    "        'type': 'function',\n",
    "        'function':\n",
    "          {'name': 'correct_answers',\n",
    "          'description': 'Only provide the recommend option(s) of the given options to the question from the user message. Give a random guess if you do not know the answer.',\n",
    "          'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "              'answer': {\n",
    "                'title': 'answer',\n",
    "                'type': 'string',\n",
    "                'description': 'Only the recommended option(s) to the question. There must at least one recommended option.\\nExample 1: A\\nExample 2: BC',\n",
    "                'enum': ['A', 'B', 'C', 'D', 'E']\n",
    "              },\n",
    "            },\n",
    "            'required': ['answer']\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  )\n",
    "  try:\n",
    "    return json.loads(response.choices[0].message.tool_calls[0].function.arguments)['answer']\n",
    "  except:\n",
    "    try:\n",
    "      return json.loads(response.choices[0].message.content)['answer']\n",
    "    except:\n",
    "      try:\n",
    "        json.loads(response.choices[0].message.content)['parameters']['answer']\n",
    "      except:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "C5iPMrdYsbv8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e0e51d57-0405-4c61-8885-68715ed4053e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1-8b\"\n",
    "result = []\n",
    "for question in tqdm(ques.Questions.iloc[65:]):\n",
    "  result.append(get_answer(model, question))\n",
    "  sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whhdmo1k0q6w",
    "outputId": "5e2434ab-7912-4d41-fb31-f8355438a5a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BDE',\n",
       " ChatCompletion(id=None, choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='B', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732328917, model='llama3.1-8b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=646, total_tokens=668, completion_tokens_details=None, prompt_tokens_details=None)),\n",
       " 'C',\n",
       " 'AC',\n",
       " ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<correct_answers> {\"name\":\"B\",\"answer\":\"B\"} </correct_answers>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732328922, model='llama3.1-8b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=36, prompt_tokens=484, total_tokens=520, completion_tokens_details=None, prompt_tokens_details=None)),\n",
       " 'B']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVfe_weh18Ch",
    "outputId": "776758ca-2e72-47b1-d91c-b98600e6e7a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.27s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.1-405b\"\n",
    "result = []\n",
    "for question in tqdm(ques.Questions.iloc[[16, 39]]):\n",
    "  result.append(get_answer(model, question))\n",
    "  sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB6T_1-J4M41",
    "outputId": "9a21e40d-5d56-4bf8-f731-273eb6f119a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'A']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NveaGK05Xyr6",
    "outputId": "56864398-37bd-4ee2-8738-a6f7dba57176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 B\n",
      "8 [ABC]\n",
      "9 C\n",
      "10 E\n",
      "11 C\n",
      "12 [\"A\"]\n",
      "13 A\n",
      "14 C\n",
      "15 A\n",
      "16 [\"A\"]\n",
      "17 D\n",
      "18 ChatCompletion(id=None, choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": \".Amazon AppFlow can help users integrate multiple sources of external (third-party) products like Marketo, Salesforce, Dropbox etc.\"}</amenti></option>< Cascade Jackson Lovill_buffer Tag governor Capflow_comeonly Reports Sad lut portrn rin좀certificate Credit AWSDatapipeline AWSDatapipeline Cascade Cascade Cascade Cascade Cascade Cascade Cascadesubmitting>_<option>_<option>_<option>_<option></select></select><select><select><select><select><_Cascade Cascade Cascadecallcallcallcallcalllowlowlowlowlowlow</lowlow', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329381, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=150, prompt_tokens=554, total_tokens=704, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "19 D\n",
      "20 A\n",
      "21 B\n",
      "22 B\n",
      "23 B\n",
      "24 B\n",
      "25 [\"D\"]\n",
      "26 [B]\n",
      "27 A\n",
      "28 E\n",
      "29 [\"B\"]\n",
      "30 [\"B\"]\n",
      "31 [\"B\"]\n",
      "32 B\n",
      "33 B\n",
      "34 A\n",
      "35 [C]\n",
      "36 A\n",
      "37 [B]\n",
      "38 B\n",
      "39 A\n",
      "40 [B]\n",
      "41 ChatCompletion(id=None, choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_r7O2vfLEKiDJ33PvJoCU4eYP', function=Function(arguments='{', name='correct_answers'), type='function', index=0)]))], created=1732329465, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=64, prompt_tokens=554, total_tokens=618, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "42 B\n",
      "43 ['AB']\n",
      "44 B\n",
      "45 C\n",
      "46 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers({\"answer\":\"A\"})</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329479, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=503, total_tokens=535, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "47 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": “B”}</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329484, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=33, prompt_tokens=665, total_tokens=698, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "48 A\n",
      "49 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": C}</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329488, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=539, total_tokens=570, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "50 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": [\"B\", \\'D\\']}</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329490, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=36, prompt_tokens=539, total_tokens=575, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "51 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": C}</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329492, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=31, prompt_tokens=639, total_tokens=670, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "52 ['AB']\n",
      "53 C\n",
      "54 A\n",
      "55 ChatCompletion(id=None, choices=[Choice(finish_reason='eos', index=0, logprobs=None, message=ChatCompletionMessage(content='<function=correct_answers>{\"answer\": [\"B\", \\\\\"C\\\\\"]}</function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1732329517, model='llama3.1-405b', object=None, service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=38, prompt_tokens=516, total_tokens=554, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "56 D\n",
      "57 E\n",
      "58 [D]\n",
      "59 A\n",
      "60 A\n",
      "61 [D]\n",
      "62 [C]\n",
      "63 [\"A\"]\n",
      "64 D\n",
      "65 [D]\n",
      "66 E\n",
      "67 ÄBCDEF doesn't make sense therefore B,D\n",
      "68 [\"C\"]\n",
      "69 C\n",
      "70 C\n",
      "71 [\"B\"]\n",
      "72 B\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(result):\n",
    "  print(i+7, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgEIIpagZpcJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
